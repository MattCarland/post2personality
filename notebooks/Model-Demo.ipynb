{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    1400 non-null   object\n",
      " 1   text    1400 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11761/635315286.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.rename(columns={'label': 'type'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data.data import Data\n",
    "\n",
    "ROWS = 200 # Limit 6000\n",
    "\n",
    "data_raw = Data().get_all_data()\n",
    "\n",
    "\n",
    "# 8670 ROWS INPUT #\n",
    "df1 = data_raw['twitter_MBTI']\n",
    "df1 = df1[['label', 'text']]\n",
    "df1.rename(columns={'label': 'type'}, inplace=True)\n",
    "df1 = df1.sample(n = ROWS, ignore_index = True)\n",
    "\n",
    "# 106062 ROWS INPUT #\n",
    "df2 = data_raw['MBTI 500']\n",
    "df2 = df2[['type', 'posts']]\n",
    "df2.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df2 = df2.sample(n = ROWS, ignore_index = True) # > 15000 combined, > 20000 df1 + df2\n",
    "\n",
    "## SPECIFCALLY CREATING DIV 0 ERROR, ASK ABOUT IT ##\n",
    "# 7811 ROWS INPUT #\n",
    "df3 = data_raw['mbti_1']\n",
    "df3.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df3 = df3.iloc[:1000]\n",
    "# df3 = df3.sample(n = ROWS, ignore_index = True)\n",
    "# ####################################################\n",
    "\n",
    "## Combined all data for PP if desired\n",
    "data_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "data_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset contains 7811 rows\n",
      "\n",
      "cleaned dataset contains 1400 rows and 8 columns\n",
      "E-I dataset contains 734 rows and 2420 columns\n",
      "S-N dataset contains 398 rows and 2514 columns\n",
      "F-T dataset contains 1392 rows and 2375 columns\n",
      "P-J dataset contains 1138 rows and 2378 columns\n"
     ]
    }
   ],
   "source": [
    "from scripts.Preprocessing_full import training_preprocessing, training_oversampling, training_balancing, training_vectorize\n",
    "\n",
    "df_pp = training_preprocessing(data_combined)\n",
    "df_pp_os = training_oversampling(df_pp)\n",
    "df_pp_os_bal = training_balancing(df_pp_os)\n",
    "df_list = training_vectorize(df_pp_os_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.model import *\n",
    "\n",
    "model_list = initialize_models(new_models = True)\n",
    "model_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) IE\n",
      "================================\n",
      "Best parameters: {'alpha': 0.0001, 'penalty': 'elasticnet'}\n",
      "Best score: 0.6315248429468875\n",
      "================================\n",
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) NS\n",
      "================================\n",
      "Best parameters: {'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Best score: 0.7625974025974026\n",
      "================================\n",
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) FT\n",
      "================================\n",
      "Best parameters: {'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Best score: 0.7022257467618293\n",
      "================================\n",
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) JP\n",
      "================================\n",
      "Best parameters: {'alpha': 0.001, 'penalty': 'elasticnet'}\n",
      "Best score: 0.5314308176100628\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alpha': 0.0001, 'penalty': 'elasticnet'},\n",
       " {'alpha': 0.0001, 'penalty': 'l1'},\n",
       " {'alpha': 0.0001, 'penalty': 'l1'},\n",
       " {'alpha': 0.001, 'penalty': 'elasticnet'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list = grid_search_all_models(df_list)\n",
    "param_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000,\n",
       "               penalty='elasticnet'),\n",
       " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1'),\n",
       " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1'),\n",
       " SGDClassifier(alpha=0.001, early_stopping=True, loss='log_loss', max_iter=5000,\n",
       "               penalty='elasticnet')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = initialize_models(params = param_list)\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 52.723%\n",
      "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000,\n",
      "              penalty='elasticnet')\n",
      "MBTI Type: IE\n",
      "========================================\n",
      "F1-score: 65.278%\n",
      "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1')\n",
      "MBTI Type: NS\n",
      "========================================\n",
      "F1-score: 58.544%\n",
      "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1')\n",
      "MBTI Type: FT\n",
      "========================================\n",
      "F1-score: 49.208%\n",
      "Model: SGDClassifier(alpha=0.001, early_stopping=True, loss='log_loss', max_iter=5000,\n",
      "              penalty='elasticnet')\n",
      "MBTI Type: JP\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "history = train_model(df_list, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENTJ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_raw['MBTI 500']\n",
    "df = df[['type', 'posts']]\n",
    "df.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df = df.sample(n = 10, ignore_index = True)\n",
    "\n",
    "estimation = df['type'].iloc[:1].tolist()[0]\n",
    "df_pred = df.iloc[:1].drop(columns =['type'])\n",
    "\n",
    "estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.Preprocessing_full import prediction_preprocessing, prediction_vectorize\n",
    "\n",
    "df_pred_pp = prediction_preprocessing(df_pred)\n",
    "df_pred_list = prediction_vectorize(df_pred_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction = ESTP\n",
      "Actual = ENTJ\n"
     ]
    }
   ],
   "source": [
    "predict_model(texts = df_pred_list, verbose = False)\n",
    "print(f\"Actual = {estimation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post2personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
