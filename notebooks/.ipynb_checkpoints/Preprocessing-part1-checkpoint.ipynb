{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197e9c55",
   "metadata": {},
   "source": [
    "## Preprocessing Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30a9be2",
   "metadata": {},
   "source": [
    "Functions should take in a dataframe with two columns: ['target'] and ['text'] (in that order), and return the same. Everything in between is up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4939826",
   "metadata": {},
   "source": [
    "### Importing and arranging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c2b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/mohammad/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/mohammad/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/mohammad/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/mohammad/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: tqdm in /home/mohammad/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa65809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310205ab",
   "metadata": {},
   "source": [
    "#### Processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cee5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"MBTI 500.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607950b3",
   "metadata": {},
   "source": [
    "#### Unprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"twitter_MBTI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(df2.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedff69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns={'text': 'posts', 'label': 'type'})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"mbti_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[['posts', 'type']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051ca7b",
   "metadata": {},
   "source": [
    "#### Combing unprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991388b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df2, df3], axis=0)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5427460",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c143f6f",
   "metadata": {},
   "source": [
    "#### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba9834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(data):\n",
    "    data = re.sub(r'http\\S+|www.\\S+', '', data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c884ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['posts'].apply(remove_urls)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece3c75e",
   "metadata": {},
   "source": [
    "#### remove social media handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_handles(data):\n",
    "    data = re.sub(r'@\\w+', '', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34caea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(remove_handles)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d2324",
   "metadata": {},
   "source": [
    "#### remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation(data):\n",
    "    for punctuation in string.punctuation:\n",
    "            data = data.replace(punctuation, '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80632526",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(punctuation)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e04fa9",
   "metadata": {},
   "source": [
    "#### lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3a0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(data):\n",
    "    return data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(lower_case)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83feed",
   "metadata": {},
   "source": [
    "#### remove special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(data):\n",
    "    data = re.sub(r'[^A-Za-z0-9\\s]+', '', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ef343",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(remove_special_characters)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ae8cb9",
   "metadata": {},
   "source": [
    "#### remove white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_space(data):\n",
    "    return data.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e755d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(white_space)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdaafa5",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    data = word_tokenize(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(tokenize)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc54a6d",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(data):\n",
    "    data = [w for w in data if w not in stop_words] \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a13f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(stopwords)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb3bcd",
   "metadata": {},
   "source": [
    "### Text Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f797336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(data):\n",
    "\n",
    "    # Lemmatizing the verbs\n",
    "    data = [WordNetLemmatizer().lemmatize(word, pos = \"v\") for word in data]\n",
    "\n",
    "    # Lemmatizing the nouns\n",
    "    data = [WordNetLemmatizer().lemmatize(word, pos = \"n\") for word in data]\n",
    "    \n",
    "    return ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b024642",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['clean_posts'] = combined_df['clean_posts'].apply(lemmatize)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21328c",
   "metadata": {},
   "source": [
    "### Testing different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66293f08",
   "metadata": {},
   "source": [
    "#### tfidf-multinominalNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54988bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import set_config; set_config(\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_df['clean_posts']\n",
    "y = combined_df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58c471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
