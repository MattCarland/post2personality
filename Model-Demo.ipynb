{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    1400 non-null   object\n",
      " 1   text    1400 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zm/96y7vfh93l90fb_pxv9n_ndm0000gn/T/ipykernel_98376/1737909153.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.rename(columns={'label': 'type'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data.data import Data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ROWS = 200 # Limit 6000\n",
    "\n",
    "data_raw = Data().get_all_data()\n",
    "\n",
    "\n",
    "# 8670 ROWS INPUT #\n",
    "df1 = data_raw['twitter_MBTI']\n",
    "df1 = df1[['label', 'text']]\n",
    "df1.rename(columns={'label': 'type'}, inplace=True)\n",
    "df1 = df1.sample(n = ROWS, ignore_index = True)\n",
    "\n",
    "# 106062 ROWS INPUT #\n",
    "df2 = data_raw['MBTI 500']\n",
    "df2 = df2[['type', 'posts']]\n",
    "df2.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df2 = df2.sample(n = ROWS, ignore_index = True) # > 15000 combined, > 20000 df1 + df2\n",
    "\n",
    "## SPECIFCALLY CREATING DIV 0 ERROR, ASK ABOUT IT ##\n",
    "# 7811 ROWS INPUT #\n",
    "df3 = data_raw['mbti_1']\n",
    "df3.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df3 = df3.iloc[:1000]\n",
    "# df3 = df3.sample(n = ROWS, ignore_index = True)\n",
    "# ####################################################\n",
    "\n",
    "## Combined all data for PP if desired\n",
    "data_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "data_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset contains 7811 rows\n",
      "\n",
      "cleaned dataset contains 1400 rows and 8 columns\n",
      "E-I dataset contains 734 rows and 2420 columns\n",
      "S-N dataset contains 398 rows and 2514 columns\n",
      "F-T dataset contains 1392 rows and 2375 columns\n",
      "P-J dataset contains 1138 rows and 2378 columns\n"
     ]
    }
   ],
   "source": [
    "from scripts.Preprocessing_full import training_preprocessing, training_oversampling, training_balancing, training_vectorize\n",
    "\n",
    "df_pp = training_preprocessing(data_combined)\n",
    "df_pp_os = training_oversampling(df_pp)\n",
    "df_pp_os_bal = training_balancing(df_pp_os)\n",
    "df_list = training_vectorize(df_pp_os_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.model import *\n",
    "\n",
    "model_list = initialize_models(new_models = True)\n",
    "model_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) IE\n",
      "================================\n",
      "Best parameters: {'alpha': 0.0001, 'penalty': 'elasticnet'}\n",
      "Best score: 0.6315248429468875\n",
      "================================\n",
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) NS\n",
      "================================\n",
      "Best parameters: {'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Best score: 0.7625974025974026\n",
      "================================\n",
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) FT\n",
      "================================\n",
      "Best parameters: {'alpha': 0.0001, 'penalty': 'l1'}\n",
      "Best score: 0.7022257467618293\n",
      "================================\n",
      "Model Searched = SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000) JP\n",
      "================================\n",
      "Best parameters: {'alpha': 0.001, 'penalty': 'elasticnet'}\n",
      "Best score: 0.5314308176100628\n",
      "================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'alpha': 0.0001, 'penalty': 'elasticnet'},\n",
       " {'alpha': 0.0001, 'penalty': 'l1'},\n",
       " {'alpha': 0.0001, 'penalty': 'l1'},\n",
       " {'alpha': 0.001, 'penalty': 'elasticnet'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_list = grid_search_all_models(df_list)\n",
    "param_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000,\n",
       "               penalty='elasticnet'),\n",
       " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1'),\n",
       " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1'),\n",
       " SGDClassifier(alpha=0.001, early_stopping=True, loss='log_loss', max_iter=5000,\n",
       "               penalty='elasticnet')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list = initialize_models(params = param_list)\n",
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 52.723%\n",
      "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000,\n",
      "              penalty='elasticnet')\n",
      "MBTI Type: IE\n",
      "========================================\n",
      "F1-score: 65.278%\n",
      "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1')\n",
      "MBTI Type: NS\n",
      "========================================\n",
      "F1-score: 58.544%\n",
      "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1')\n",
      "MBTI Type: FT\n",
      "========================================\n",
      "F1-score: 49.208%\n",
      "Model: SGDClassifier(alpha=0.001, early_stopping=True, loss='log_loss', max_iter=5000,\n",
      "              penalty='elasticnet')\n",
      "MBTI Type: JP\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "history = train_model(df_list, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ENTP'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_raw['MBTI 500']\n",
    "df = df[['type', 'posts']]\n",
    "df.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df = df.sample(n = 10, ignore_index = True)\n",
    "\n",
    "estimation = df['type'].iloc[:1].tolist()[0]\n",
    "df_pred = df.iloc[:1].drop(columns =['type'])\n",
    "\n",
    "estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericferole/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericferole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericferole/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mwordnet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m df_pred_pp \u001b[39m=\u001b[39m prediction_preprocessing(df_pred)\n\u001b[1;32m      8\u001b[0m df_pred_list \u001b[39m=\u001b[39m prediction_vectorize(df_pred_pp)\n",
      "File \u001b[0;32m~/code/post2personality/scripts/Preprocessing_full.py:642\u001b[0m, in \u001b[0;36mprediction_preprocessing\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[39mUse this function to clean USER data, for feeding into the production model\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[39min order to generate PREDICTIONS.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39mpresent.)\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m# Text cleaning, etc. (all input/output == STR format)\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m data \u001b[39m=\u001b[39m lowercasing(data)\n\u001b[1;32m    643\u001b[0m data \u001b[39m=\u001b[39m remove_separators(data)\n\u001b[1;32m    644\u001b[0m data \u001b[39m=\u001b[39m remove_urls(data)\n",
      "File \u001b[0;32m~/code/post2personality/scripts/Preprocessing_full.py:197\u001b[0m, in \u001b[0;36mlowercasing\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlower_case\u001b[39m(text):\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39mlower()\n\u001b[0;32m--> 197\u001b[0m dataframe[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m dataframe[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(lower_case)\n\u001b[1;32m    198\u001b[0m \u001b[39mreturn\u001b[39;00m dataframe\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages/pandas/core/series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4517\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4522\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4523\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4524\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4525\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4624\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4626\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/post2personality/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/code/post2personality/scripts/Preprocessing_full.py:196\u001b[0m, in \u001b[0;36mlowercasing.<locals>.lower_case\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlower_case\u001b[39m(text):\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39;49mlower()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from scripts.Preprocessing_full import prediction_preprocessing, prediction_vectorize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df_pred_pp = prediction_preprocessing(df_pred)\n",
    "df_pred_list = prediction_vectorize(df_pred_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction = ESTP\n",
      "Actual = ENTJ\n"
     ]
    }
   ],
   "source": [
    "predict_model(texts = df_pred_list, verbose = False)\n",
    "print(f\"Actual = {estimation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post2personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
