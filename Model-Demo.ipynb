{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    1400 non-null   object\n",
      " 1   text    1400 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.0+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zm/96y7vfh93l90fb_pxv9n_ndm0000gn/T/ipykernel_98596/1737909153.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1.rename(columns={'label': 'type'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data.data import Data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ROWS = 200 # Limit 6000\n",
    "\n",
    "data_raw = Data().get_all_data()\n",
    "\n",
    "\n",
    "# 8670 ROWS INPUT #\n",
    "df1 = data_raw['twitter_MBTI']\n",
    "df1 = df1[['label', 'text']]\n",
    "df1.rename(columns={'label': 'type'}, inplace=True)\n",
    "df1 = df1.sample(n = ROWS, ignore_index = True)\n",
    "\n",
    "# 106062 ROWS INPUT #\n",
    "df2 = data_raw['MBTI 500']\n",
    "df2 = df2[['type', 'posts']]\n",
    "df2.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df2 = df2.sample(n = ROWS, ignore_index = True) # > 15000 combined, > 20000 df1 + df2\n",
    "\n",
    "## SPECIFCALLY CREATING DIV 0 ERROR, ASK ABOUT IT ##\n",
    "# 7811 ROWS INPUT #\n",
    "df3 = data_raw['mbti_1']\n",
    "df3.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df3 = df3.iloc[:1000]\n",
    "# df3 = df3.sample(n = ROWS, ignore_index = True)\n",
    "# ####################################################\n",
    "\n",
    "## Combined all data for PP if desired\n",
    "data_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "data_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INFJ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_raw['MBTI 500']\n",
    "df = df[['type', 'posts']]\n",
    "df.rename(columns={'posts': 'text'}, inplace=True)\n",
    "df = df.sample(n = 10, ignore_index = True)\n",
    "\n",
    "estimation = df['type'].iloc[:1].tolist()[0]\n",
    "df_pred = df.iloc[:1].drop(columns =['type'])\n",
    "\n",
    "estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ericferole/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ericferole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ericferole/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ei_vectorizer.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mwordnet\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m df_pred_pp \u001b[39m=\u001b[39m prediction_preprocessing(df_pred)\n\u001b[0;32m----> 8\u001b[0m df_pred_list \u001b[39m=\u001b[39m prediction_vectorize(df_pred_pp)\n",
      "File \u001b[0;32m~/code/post2personality/scripts/Preprocessing_full.py:672\u001b[0m, in \u001b[0;36mprediction_vectorize\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprediction_vectorize\u001b[39m(data):\n\u001b[1;32m    665\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[39m    Uses pre-fitted vectorizers, saved locally as .pkl files, to transform user\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m    data in order to feed into models for trait prediction.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[39m    factor.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     e_i_data \u001b[39m=\u001b[39m ei_vectorize(data, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    673\u001b[0m     s_n_data \u001b[39m=\u001b[39m sn_vectorize(data, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    674\u001b[0m     f_t_data \u001b[39m=\u001b[39m ft_vectorize(data, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/post2personality/scripts/Preprocessing_full.py:362\u001b[0m, in \u001b[0;36mei_vectorize\u001b[0;34m(dataframe, is_train)\u001b[0m\n\u001b[1;32m    360\u001b[0m     ei_X \u001b[39m=\u001b[39m ei_vectorizer\u001b[39m.\u001b[39mtransform(dataframe[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin))\n\u001b[1;32m    361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 362\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mei_vectorizer.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    363\u001b[0m         ei_vectorizer \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m    364\u001b[0m     ei_X \u001b[39m=\u001b[39m ei_vectorizer\u001b[39m.\u001b[39mtransform(dataframe[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ei_vectorizer.pkl'"
     ]
    }
   ],
   "source": [
    "from scripts.Preprocessing_full import prediction_preprocessing, prediction_vectorize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df_pred_pp = prediction_preprocessing(df_pred)\n",
    "df_pred_list = prediction_vectorize(df_pred_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prediction = ESTP\n",
      "Actual = ENTJ\n"
     ]
    }
   ],
   "source": [
    "predict_model(texts = df_pred_list, verbose = False)\n",
    "print(f\"Actual = {estimation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post2personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
