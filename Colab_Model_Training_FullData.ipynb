{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "bFxwgG7FFlZq"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### GLOBAL ###########\n",
        "\n",
        "NLTK_ERROR_HAPPENING = False\n",
        "\n",
        "MAKE_DATA = False\n",
        "ROWS = 70000 # LIMIT = 106062\n",
        "DF1, DF2, DF3 = False, True, False\n",
        "ALL_ROWS = False\n",
        "\n",
        "PREPROCESS = False\n",
        "\n",
        "MODEL_TYPE = 'SGD'\n",
        "\n",
        "NEW_MODELS = False\n",
        "\n",
        "PARAMETER_SEARCH = False\n",
        "\n",
        "TRAIN = False\n",
        "\n",
        "PREDICT_TEST = False\n",
        "\n",
        "###### GLOBAL ###########"
      ],
      "metadata": {
        "id": "7HZ1m4t7MWt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mq9Xvxoh_-vw"
      },
      "outputs": [],
      "source": [
        "if NLTK_ERROR_HAPPENING == True\n",
        "  import nltk\n",
        "  nltk.download('punkt')\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('wordnet')\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from data.data import Data\n",
        "from scripts.Preprocessing_full import *\n",
        "from model.model import *\n",
        "\n",
        "\n",
        "if MAKE_DATA == True:\n",
        "  data_raw = Data().get_all_data()\n",
        "\n",
        "  # 8670 ROWS INPUT #\n",
        "  if DF1:\n",
        "    df1 = data_raw['twitter_MBTI']\n",
        "    df1 = df1[['label', 'text']]\n",
        "    df1.rename(columns={'label': 'type'}, inplace=True)\n",
        "    if ALL_ROWS == False:\n",
        "      df1 = df1.sample(n = ROWS, ignore_index = True)\n",
        "\n",
        "\n",
        "  # 106062 ROWS INPUT #\n",
        "  if DF2:\n",
        "    df2 = data_raw['MBTI 500']\n",
        "    df2 = df2[['type', 'posts']]\n",
        "    df2.rename(columns={'posts': 'text'}, inplace=True)\n",
        "    if ALL_ROWS == False:\n",
        "      df2 = df2.sample(n = ROWS, ignore_index = True) \n",
        "\n",
        "\n",
        "  # 7811 ROWS INPUT #\n",
        "  if DF3:\n",
        "    df3 = data_raw['mbti_1']\n",
        "    df3.rename(columns={'posts': 'text'}, inplace=True)\n",
        "    if ALL_ROWS == False:\n",
        "      df3 = df3.sample(n = ROWS, ignore_index = True)\n",
        "\n",
        "  #####################################################\n",
        "\n",
        "\n",
        "\n",
        "  ## Combined all data for PP if desired\n",
        "  data_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "else:\n",
        "  print(\"Data ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.Preprocessing_full import training_preprocessing, training_oversampling, training_balancing, training_vectorize\n",
        "\n",
        "if PREPROCESS:\n",
        "  df_pp = training_preprocessing(data_combined)\n",
        "  df_pp_os = training_oversampling(df_pp)\n",
        "  df_pp_os_bal = training_balancing(df_pp_os)\n",
        "  df_list = training_vectorize(df_pp_os_bal)\n",
        "\n",
        "  df_list[0].dropna(inplace = True)\n",
        "  df_list[1].dropna(inplace = True)\n",
        "  df_list[2].dropna(inplace = True)\n",
        "  df_list[3].dropna(inplace = True)\n",
        "\n",
        "  with open(f'dflist_pp_{ROWS}.pkl', 'wb') as file:\n",
        "    pickle.dump(df_list, file)\n",
        "\n",
        "else:\n",
        "  with open(f'dflist_pp_{ROWS}.pkl', 'rb') as file:\n",
        "    df_list = pickle.load(file)\n",
        "  print(\"df_list loaded\")"
      ],
      "metadata": {
        "id": "--zajHjRCPc0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!--  -->"
      ],
      "metadata": {
        "id": "XSjxhLpgJQsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if NEW_MODELS:\n",
        "  model_list = initialize_models(new_models = True,\n",
        "                               params = None,\n",
        "                               model_type = MODEL_TYPE)\n",
        "  print(\"model_list is NEW\")\n",
        "else:\n",
        "  model_list = load_models_pkl()\n",
        "  print(\"model_list is OLD\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_z6_NvOCR8K",
        "outputId": "e71d7d20-e320-4d48-f01a-e152fec19ed8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000),\n",
              " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000),\n",
              " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000),\n",
              " SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000)]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if PARAMETER_SEARCH:\n",
        "  param_list = grid_search_all_models(df_list, \n",
        "                          model_type = MODEL_TYPE)\n",
        "  with open(f'param_list_sgd.pkl', 'wb') as file:\n",
        "    pickle.dump(param_list, file)\n",
        "  \n",
        "  model_list = initialize_models(params = param_list,\n",
        "                               model_type=MODEL_TYPE)\n",
        "  print(\"model_list is optimized on new params\")\n",
        "  \n",
        "else:\n",
        "  with open(f'param_list_sgd.pkl', 'rb') as file:\n",
        "    param_list = pickle.load(file)\n",
        "\n",
        "  model_list = initialize_models(params = param_list,\n",
        "                               model_type=MODEL_TYPE) \n",
        "  \n",
        "  print(\"model_list is optimized on old params\")"
      ],
      "metadata": {
        "id": "w_LKOyI9CT_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "  history = train_model(df_list)\n",
        "else:\n",
        "  print(\"Not Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z-yfDLeCajV",
        "outputId": "21b60855-e533-4a6d-ff26-6b3a93977fdb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 72.395%\n",
            "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1')\n",
            "MBTI Type: EI\n",
            "========================================\n",
            "F1-score: 77.11%\n",
            "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000, penalty='l1')\n",
            "MBTI Type: NS\n",
            "========================================\n",
            "F1-score: 79.228%\n",
            "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000,\n",
            "              penalty='elasticnet')\n",
            "MBTI Type: FT\n",
            "========================================\n",
            "F1-score: 65.688%\n",
            "Model: SGDClassifier(early_stopping=True, loss='log_loss', max_iter=5000,\n",
            "              penalty='elasticnet')\n",
            "MBTI Type: JP\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.Preprocessing_full import prediction_preprocessing, prediction_vectorize\n",
        "if not MAKE_DATA:\n",
        "  data_raw = Data().get_all_data()\n",
        "  df2 = data_raw['MBTI 500']\n",
        "  df2 = df2[['type', 'posts']]\n",
        "  df2.rename(columns={'posts': 'text'}, inplace=True)\n",
        "\n",
        "if PREDICT_TEST:\n",
        "  j = 0\n",
        "  for i in range(100):\n",
        "    df = df2.sample(n = 10, ignore_index = True)\n",
        "    estimation = df['type'].iloc[:1].tolist()[0]\n",
        "    df_pred = df.iloc[:1].drop(columns =['type'])\n",
        "\n",
        "    df_pred_pp = prediction_preprocessing(df_pred)\n",
        "    df_pred_list = prediction_vectorize(df_pred_pp)\n",
        "    pred = predict_model(texts = df_pred_list, verbose = False)\n",
        "    # print(f\"Prediction = {pred['type_prediction']} {pred['type_score']}\")\n",
        "    # print(f\"Actual = {estimation}\")\n",
        "    if pred['type_prediction'] == estimation:\n",
        "      j += 1\n",
        "  print(f\"{j} correct out of {i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIn2fmOYCg8s",
        "outputId": "4e03278c-df62-4624-c111-f1cd5e085209"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37 correct out of 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DF Comparison (n=7000)\n",
        "### DF1 ------------------------------------------\n",
        "F1-score: 52.669999999999995%\n",
        "MBTI Type: EI\n",
        "\n",
        "F1-score: 53.102000000000004%\n",
        "MBTI Type: NS\n",
        "\n",
        "F1-score: 49.124%\n",
        "MBTI Type: TF\n",
        "\n",
        "F1-score: 57.786% \n",
        "MBTI Type: PJ\n",
        "\n",
        "### DF2 ------------------------------------------\n",
        "F1-score: 60.718%\n",
        "MBTI Type: EI\n",
        " \n",
        "F1-score: 60.75000000000001%\n",
        "MBTI Type: NS \n",
        "\n",
        "F1-score: 78.408%\n",
        "MBTI Type: TF\n",
        "\n",
        "F1-score: 54.04600000000001%\n",
        "MBTI Type: PJ\n",
        "\n",
        "\n",
        "### DF 3 -------------------------------------\n",
        "\n",
        "\n",
        "F1-score: 58.536%\n",
        "MBTI Type: IE\n",
        "\n",
        "F1-score: 58.318999999999996%\n",
        "MBTI Type: NS\n",
        "\n",
        "F1-score: 66.681%\n",
        "MBTI Type: TF\n",
        "\n",
        "F1-score: 57.265%\n",
        "MBTI Type: JP\n",
        "\n"
      ],
      "metadata": {
        "id": "xfVFzbNNmo4j"
      }
    }
  ]
}