{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals for automatic nature\n",
    "MAKE_DATA = True\n",
    "ROWS = 500\n",
    "\n",
    "MAKE_PREPROCESS = True\n",
    "LOAD_PREPROCESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from data.data import Data\n",
    "\n",
    "if MAKE_DATA:\n",
    "    data_raw = Data().get_all_data()\n",
    "    df = data_raw['MBTI 500']\n",
    "\n",
    "    df = df[['type', 'posts']]\n",
    "    df.rename(columns={'posts': 'text'}, inplace=True)\n",
    "    df = df.sample(n = ROWS, ignore_index = True)\n",
    "\n",
    "    print(f\"MBTI_500 loaded with shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Preprocessing\n",
    "from scripts.BERT_preprocessing import bert_training_preprocessing\n",
    "\n",
    "if MAKE_PREPROCESS:\n",
    "    df_list = bert_training_preprocessing(df)\n",
    "    with open(f'data/prepro/BERT_pp_{ROWS}.pkl', 'wb') as file:\n",
    "            pickle.dump(df_list, file)\n",
    "            \n",
    "    with open(f'data/prepro/BERT_pp_{ROWS}.pkl', 'rb') as file:\n",
    "            df_list = pickle.load(file)\n",
    "            \n",
    "    print(f\"Made BERT PrePro set of size {df_list[0]}\")\n",
    "            \n",
    "if LOAD_PREPROCESS:\n",
    "    with open(f'data/prepro/BERT_pp_{ROWS}.pkl', 'rb') as file:\n",
    "            df_list = pickle.load(file)\n",
    "    print(f\"Loaded BERT PrePro set of size {df_list[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xlist = []\n",
    "for i in range(len(df_list[0]['embeddings'])):\n",
    "    z = np.array(df_list[0]['embeddings'][i])\n",
    "    z_pad = np.pad(z, ((0, 400 - z.shape[0]), (0, 0)), 'constant', constant_values=-1000)\n",
    "    Xlist.append(z_pad)\n",
    "X_ug = np.vstack(Xlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_list[0].iloc[:,[0]]\n",
    "X = X_ug.reshape(y.shape[0],-1,768)\n",
    "print(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "label_as_binary = LabelBinarizer()\n",
    "ylabels = label_as_binary.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Masking(mask_value=-1000))\n",
    "\n",
    "#1. Normalization\n",
    "model.add(layers.Normalization())\n",
    "\n",
    "#2. SimpleRNN layer, 20 units, tanh\n",
    "model.add(layers.LSTM(units = 25, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(512, activation='sigmoid', kernel_regularizer='l1'))\n",
    "\n",
    "model.add(layers.Dropout(.30))\n",
    "\n",
    "model.add(layers.Dense(512//2, activation='sigmoid', kernel_regularizer='l1'))\n",
    "model.add(layers.Dense(512//4, activation='sigmoid', kernel_regularizer='l1'))\n",
    "model.add(layers.Dense(512//8, activation='sigmoid', kernel_regularizer='l1'))\n",
    "\n",
    "model.add(layers.Dropout(.50))\n",
    "\n",
    "model.add(layers.Dense(32, activation='softmax', kernel_regularizer='l2'))\n",
    "\n",
    "\n",
    "#4 Dense Layer with Return Layer\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', \n",
    "              optimizer='Adam',\n",
    "              metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 8s 275ms/step - loss: 75.4270 - mse: 0.2397 - val_loss: 56.4744 - val_mse: 0.2788\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 6s 257ms/step - loss: 42.9848 - mse: 0.2082 - val_loss: 29.4703 - val_mse: 0.3169\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 6s 248ms/step - loss: 20.4082 - mse: 0.1778 - val_loss: 12.4094 - val_mse: 0.3547\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 6s 256ms/step - loss: 7.8817 - mse: 0.1548 - val_loss: 5.2476 - val_mse: 0.3857\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 6s 250ms/step - loss: 3.2691 - mse: 0.1384 - val_loss: 2.6341 - val_mse: 0.4107\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 6s 249ms/step - loss: 1.5887 - mse: 0.1257 - val_loss: 1.8256 - val_mse: 0.4330\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 6s 255ms/step - loss: 0.9732 - mse: 0.1147 - val_loss: 1.5108 - val_mse: 0.4525\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 6s 248ms/step - loss: 0.7310 - mse: 0.1049 - val_loss: 1.4640 - val_mse: 0.4694\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 6s 279ms/step - loss: 0.6688 - mse: 0.0975 - val_loss: 1.4770 - val_mse: 0.4850\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 6s 249ms/step - loss: 0.6387 - mse: 0.0907 - val_loss: 1.5026 - val_mse: 0.4994\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 6s 252ms/step - loss: 0.6178 - mse: 0.0853 - val_loss: 1.5305 - val_mse: 0.5132\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 6s 250ms/step - loss: 0.5982 - mse: 0.0793 - val_loss: 1.5568 - val_mse: 0.5260\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 6s 250ms/step - loss: 0.5809 - mse: 0.0742 - val_loss: 1.5843 - val_mse: 0.5382\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience = 5)\n",
    "\n",
    "history = model.fit(X, ylabels,\n",
    "                  validation_split = .5,\n",
    "                  epochs = 50,\n",
    "                  batch_size = 32,\n",
    "                  callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "## n = 500\n",
    "loss: 0.5602 - val_loss: 1.1611, opt = Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Load Models\n",
    "\n",
    "DIE\n",
    "\n",
    "from model.DL import *\n",
    "\n",
    "model_list = initialize_models(metrics='accuracy',\n",
    "                               loss='binary_crossentropy',\n",
    "                               vocab_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "train_models(df_list, model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "post2personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
